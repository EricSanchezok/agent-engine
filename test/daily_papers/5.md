### Long Chain-of-Thought Reasoning Across Languages
**信号源：** University of California, Berkeley  
**论文链接：** https://arxiv.org/abs/2508.14828  

---

#### 一句话总结  
作者通过将两套英语长链式推理数据集翻译为法语、日语、拉脱维亚语与斯瓦希里语，并在 Qwen2.5-7B 和 Qwen3-8B-Base 上展开系统实验，揭示“英语枢纽”策略的适用边界，量化多语预训练与小规模监督微调对低资源语言推理能力的显著补益。

---

#### 论文摘要  
大规模语言模型在英语环境下借助长链式思考（CoT）已展现出色的推理能力，但跨语言效果尚未充分验证。论文将 s1k 与 Bespoke-Stratos-17k 自动翻译成法、日、拉、斯四语，形成 M-s1k（1 k 条）与 M-BS17k（17 k 条），并在此基础上微调 Qwen2.5-7B 与 Qwen3-8B-Base。核心结论：  
1. “英语枢纽”效果随语言而异——对法语几乎无益，对日语和拉脱维亚语有明显提升，对斯瓦希里语则作用有限。  
2. Qwen3 的 119 语言大规模预训练缩小了跨语差距，但对低资源语言仍需仅 1 k 条监督微调即可带来超过 30 个百分点的提升。  
3. 数据质量与规模的取舍具语言依赖性：高资源语种（英语、法语）侧重小而精，低资源语种（拉脱维亚语、斯瓦希里语）则受益于更大但略含噪的语料。  

---

#### 核心方法  
• 数据集构建：  
&nbsp;&nbsp;– 使用 Gemini 2.0 Flash 翻译 s1k 与 Bespoke-Stratos-17k，得到五语并行的 M-s1k、M-BS17k。  

• 模型与训练：  
&nbsp;&nbsp;– 选用 Qwen2.5-7B、Qwen3-8B-Base；  
&nbsp;&nbsp;– 在 LLaMA-Factory 框架下进行语言特定的监督微调（SFT），并以“语言强制”(language forcing) 作为无额外微调的对照。  

• 评测设置：  
&nbsp;&nbsp;– 基准集：MATH-500、AIME 2024、IMO、GPQA；  
&nbsp;&nbsp;– 统一推理参数：temperature 0.6，top-p 0.95，max tokens 16 384；  
&nbsp;&nbsp;– 采用 GPT-4.1-Mini 自动比对答案，报告准确率。  

• 关键实验设计：  
&nbsp;&nbsp;1. 英语枢纽效应：构造“问题语言 X / 推理语言 Y”混合语料，分离输入理解与推理生成影响。  
&nbsp;&nbsp;2. 多语预训练分析：对比 Qwen2.5（29 语、18 T tokens）与 Qwen3（119 语、36 T tokens）在仅提示控制 vs. 语言特定 SFT 下的差异。  
&nbsp;&nbsp;3. 数据规模探索：同一模型分别在 M-s1k 与 M-BS17k 上微调，比较不同语言对数据规模/质量的敏感性。  

---

#### 实验成果  
• 英语枢纽并非通用良方  
&nbsp;&nbsp;– 在 MATH-500 上，Qwen2.5-7B 对法语几乎无提升（76.0 % → 76.4 %）；  
&nbsp;&nbsp;– 对日语与拉脱维亚语，使用英语推理分别提升约 17 与 14 个百分点；  
&nbsp;&nbsp;– 对斯瓦希里语提升不足 1 个百分点（52.4 % → 52.6 %）。  

• 多语预训练 + 小样本微调补齐低资源差距  
&nbsp;&nbsp;– Qwen3-8B-Base 在斯瓦希里语上：仅语言强制 39.4 %，加入 1 k 条斯瓦希里 SFT 后跃升至 73.2 %（+33.8 pp）。  

• 数据规模结论  
&nbsp;&nbsp;– 英语、法语：M-s1k 已足够，扩大到 M-BS17k 收益有限；  
&nbsp;&nbsp;– 拉脱维亚语、斯瓦希里语：M-BS17k 能显著优于 M-s1k，说明低资源语种更依赖规模。  

---

#### 总结与反思  
1. 论文系统比较了长 CoT 的跨语言迁移机制，提出“高 / 中 / 低 资源语言三分层”规律。  
2. 多语大规模预训练与极小监督微调互补：前者降低门槛，后者在低资源场景仍不可或缺。  
3. 局限性：  
&nbsp;&nbsp;– 自动翻译的数据仍含噪或文化偏差；  
&nbsp;&nbsp;– 仅覆盖四种非英语语言，难以概括全部语言谱系；  
&nbsp;&nbsp;– 评测完全依赖自动判分，仍需人工复核。  

---

#### 前沿展望  
• 扩展至更多低资源语言，或探索“自译-自检”生成高质量多语推理数据。  
• 企业可基于小规模本地化微调快速部署多语推理助手，降低成本。  
• 研究层面强调“数据-预训练-微调”协同，而非单一依赖英语枢纽或盲目扩大模型规模。