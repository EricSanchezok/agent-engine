### FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy  
**信号源：** 上海交通大学  

---

#### 一句话总结  
“先流再模仿”(FBI) 通过动态视觉-触觉融合与一步 Shortcut 扩散策略，让机器人即使没有真实触觉传感器，也能以约 20 Hz 实时完成高难度翻转和推握操作；在总体平均上较最佳基线提升约 16 %–18 %，在最难的翻转任务中提升超过 20 %。  

---

#### 论文摘要  
复杂接触动力学与局部可见性长期制约单手灵巧操作。FBI 框架以“动态融合”思路将两帧点云的运动流与触觉信号建立因果联系：  
1. **Flow2Tactile** 模块先由点云流预测触觉（可选地用真实传感器微调）；  
2. 预测/实测触觉与视觉特征经 **Transformer** 融合；  
3. **一步 Shortcut Diffusion Policy** 直接输出完整动作序列，实现实时控制。  

在两类自定义任务（翻转、推移）与 Adroit 基准三任务中，FBI 在仿真和真实环境均超越四个最新基线，平均成功率提升 16.6 %（Vision-Only）至 18.4 %（Visuotactile），其中翻转任务最高提升 21.4 %。  

---

#### 核心方法  
* **整体流程**  
  1. **多模态编码**  
     - 两帧关节状态、点云与触觉读数分别经 MLP 编码为状态、视觉及触觉特征。  
  2. **动态触觉推断**  
     - Flow2Tactile 先用一步 Shortcut 模型预测点云运动流，再在 456 个手掌关键点上搜索生成稠密二值接触图，可在 “纯视觉” 模式中替代真实传感器。  
  3. **Transformer 融合**  
     - 以视觉特征为模板、触觉特征为查询，输出联合视触特征。  
  4. **一步扩散策略**  
     - Shortcut Policy 仅一次前向传播即可把高斯噪声动作去噪成完整动作序列，带来毫秒级推理。  

* **关键技术细节**  
  1. **Flow2Tactile 精度**：融合运动流与手部关键点位置，触觉预测准确率达 85.5 %。  
  2. **稠密二值触觉表示**：每个关键点仅判定“接触/未接触”，简化学习并便于传感器映射，保证仿真-现实一致性。  
  3. **Shortcut Policy 训练**：同时最小化流匹配与自一致损失，使原需多步去噪的扩散过程收敛到单步；单步去噪仅 ≈10 ms，整体模型推理 ≈34 ms。  

---

#### 实验成果  
* **成功率显著提升**  
  - 仿真五任务平均成功率：64.7 %（Vision-Only）/66.5 %（Visuotactile），分别比基线 DP3 高 16.6 % 与 18.4 %；在最困难的翻转任务提升 19.3 %–21.4 %。  
  - 真实机器人实验平均成功率：33.5 %（Vision-Only）/35.0 %（Visuotactile），较最佳基线 ManiCM 提高约 15 %–16.5 %。  

* **实时控制效率**  
  - RTX 4090 上，模型推理 34 ms（其中 Flow2Tactile 17 ms，策略生成 15 ms），加上环境交互总体约 51 ms，可实现 ≈20 Hz 闭环控制；相比 DP3 138 ms、10 步 DDIM 261 ms 延迟显著降低。  

---

#### 总结与反思  
* **成果**：FBI 以“先预测运动流→推断触觉→一步出动作”的链式设计，将视觉与触觉动态耦合，显著增强复杂接触场景下的稳定性与泛化性，并证实“无触觉硬件也可享触觉收益”的可行性。  
* **局限**：  
  - 依赖点云质量，强遮挡或深度噪声会削弱 Flow2Tactile 准确性；  
  - 二值接触无法表达细微力变化，限制极精细操作；  
  - 训练需大量成功示例（≈5000 条/物体），数据采集成本高。  
* **前沿展望**：  
  1. 将隐式触觉估计扩展到力-矩连续空间，结合语义意图实现多任务通用操控；  
  2. 对成本敏感的工业手可少装或无装触觉硬件，借视觉补偿完成高精度装配、分拣等任务；  
  3. 一步扩散结合多模态动态融合为实时端到端策略提供新范式，有望推动视-触感知从“静态拼接”迈向“因果建模”。