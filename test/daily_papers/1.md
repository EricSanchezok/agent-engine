### Towards Source-Free Machine Unlearning  

**一句话总结**  
作者首次针对线性（及混合线性）分类器，在“源数据不可用”场景下，仅凭已训练模型与待删除样本，即可高效并带有理论保证地完成实例级机器遗忘。  

---

#### 论文摘要  
随着隐私法规收紧，模型在用户要求下删除特定训练信息的需求激增。现有机器遗忘方法大多仍需访问剩余训练数据，但实际部署中往往已无法再次存储或调用这些数据。论文提出“Source-Free Machine Unlearning”：在完全看不到剩余数据的情况下执行遗忘的通用框架。作者聚焦 ℓ₂ 正则化的线性模型及可微凸损失函数，通过对“剩余数据” Hessian 的近似估计，构造一次近似 Newton 更新来移除指定样本对参数的影响；并给出参数不可区分性的显式上界。实验证明，在 CIFAR-10/100、Stanford Dogs、Caltech-256 等数据集上，新方法几乎与重新训练等价，并显著优于现有源数据缺失的遗忘技术。  

---

#### 核心方法  

1. **问题设定**  
   * 已训练参数 \(w^\star\)，待遗忘数据集 \(D_f\)；剩余训练集 \(D_r\) 不再可用。  
   * 目标：产出 \(w_{\text{uf}}\)，使其在 \(D_f\) 上“遗忘”，同时在 \(D_r\) 与测试集上的表现接近用 \(D_r\) 重新训练的模型。  

2. **Hessian 近似**  
   * 在 \(w^\star\) 周围采样多组小扰动 \(\delta w\)，分别计算遗忘集上的损失差 \(\delta L_f\)。  
   * 假设在局部邻域内 \(\delta L_f \approx \delta L_r\)，将  
     \[
       f_i(H)=\tfrac12\delta w_i^\top H\delta w_i-\nabla_f^\top\delta w_i-\delta L_f(\!w_i)
     \]  
     的平方残差最小化：\(\Psi(H)=\frac1m\sum_i f_i(H)^2\)。  
   * 令 \(H\succeq0\) 并将最小化问题写成半定规划 (SDP)，得到保正定的 Hessian 估计 \(\hat H_r\)。  

3. **一次参数更新**  
   * 用估计量执行  
     \[
       w_{\text{uf}} = w^\star - \hat H_r^{\!-1}\nabla_f + \sigma\varepsilon,
     \]  
     其中 \(\varepsilon\sim\mathcal N(0,I)\) 用于抵消可能的信息泄漏。  
   * 论文推导了 \(\|\nabla L(w_{\text{uf}},D_r)\|\) 的显式上界，表明维度增大时误差收敛、遗忘更充分。  

4. **扩展到深度网络**  
   * 采用“混合线性”策略：冻结大部分网络，仅线性化末若干层（NTK 近似），将深度模型退化为上述线性情形，再应用同样的 Hessian 估计与更新步骤。  

---

#### 实验结果  

| 数据集 | 模型设定 | 重训 (Test) | Unlearned(+) | Unlearned(-) | MIA ↓ |
|-------|----------|------------|--------------|--------------|-------|
| CIFAR-10 | 线性 | 72% | 70.3% | 70.0% | 51.5% |
| CIFAR-100 | 线性 | 56.0% | 49.8% | 51.6% | 51.8% |
| Stanford Dogs | 线性 | 59.3% | 54.6% | 55.0% | 50.9% |
| Caltech-256 | 线性 | 54.4% | 47.6% | 49.4% | 50.6% |

* Unlearned(+)：在可访问 \(D_r\) 时用真 Hessian 进行遗忘。  
* Unlearned(−)：本文方法，仅用 \(w^\star\)+\(D_f\)。  

主要观察：  
• 在四个数据集上，Unlearned(−) 的测试性能距离重训模型差距通常小于 2–3%，显著好于现有源数据缺失方法 (>15% 差距)。  
• MIA 分数从 ~59% 降至约 50%，表明模型已无法区分被删样本是否参与训练，实现有效遗忘。  

---

#### 总结与反思  

• **贡献**：提出首个针对线性及混合线性分类器、带有显式理论保证的源数据缺失机器遗忘算法；通过随机微扰与 SDP 估计 Hessian，使零样本遗忘成为可能。  
• **局限**：  
  – 依赖凸损失与 Hessian 可正定假设；端到端非凸网络仍需线性化近似。  
  – SDP 在超高维情形计算／存储成本较高；作者建议使用对角或低秩近似作为未来工作。  
  – 随遗忘样本比例增大，理论界限变宽，实验亦显示性能下降。  
• **前沿展望**：  
  – 将该框架扩展到大规模语言模型、扩散模型等非线性场景；结合 Kronecker 或低秩 Hessian 逼近可缓解计算瓶颈。  
  – 为工业场景提供可落地的“Forget API”，满足 GDPR“被遗忘权”诉求。  
  – 参数不可区分性上界为遗忘质量提供了可量化指标，有望成为后续隐私合规审计参考标准。