### DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization  
**信号源：** ByteDance Seed、南京大学  

---

#### 一句话总结  
DuPO 提出一种“已知-未知”分解的广义对偶学习框架，让大模型自己给自己打分，实现无需人工标注、跨任务通用的强化学习优化。

---

#### 论文摘要  
当前的 RLHF 依赖昂贵的人类偏好，RLVR 又受限于必须可验证的任务，传统对偶学习则要求严格可逆，难以覆盖多数开放式场景。DuPO 通过将原始输入拆分为“已知部分” x_k 与“未知部分” x_u，仅要求对偶任务重建未知部分，从而绕过“完全可逆”的硬约束，并大幅降低对偶任务难度。一旦重建成功，即可把重建误差转化为自监督奖励，并用 GRPO 等策略梯度方法反向优化原任务。实验证明，该方法在 756 个翻译方向上平均提高 2.13 COMET，在三大数学推理基准上平均提升 6.4 个百分点；在推理时充当重排序器，还可把小模型性能额外提升 9.3 点，展示了 DuPO 的通用性与可扩展性。

---

#### 核心方法  
1. **输入拆分**：将原任务输入 x 划分为已知 x_k 与未知 x_u。  
2. **原任务（Primal）**：模型根据完整输入生成输出 y。  
3. **对偶任务（Dual）**：模型利用 y 与 x_k 重建 x_u，而非重建整个 x。  
4. **奖励计算**：用重建误差 d(x_u, \hat{x}_u) 作为自监督奖励，误差越小奖励越高。  
5. **偏好优化**：通过 GRPO（或兼容的 PPO 系列算法）更新模型，使期望奖励最大化。  

技术要点：  
- **已知-未知选择策略**：  
  • 数学推理任务中自动将题目中部分数字替换为变量；  
  • 翻译任务中采用回译（反向翻译）保证语义一致性。  
- **单模型双角色**：同一 LLM 同时执行原任务与对偶任务，避免双模型能力不对称。  
- **训练-推理两用**：训练阶段提供奖励信号；推理阶段用对偶准确率对候选答案重排序，可即时提升输出质量。

---

#### 实验成果  

| 任务 | 基线模型 | DuPO 提升 | 结果摘要 |
| ---- | -------- | --------- | -------- |
| 多语翻译（28 语 756 方向） | Seed-X-7B-Instruct | +2.13 COMET | 综合得分 64.66，接近 DeepSeek-R1-0528（64.80），并超越部分超大闭源模型 |
| 数学推理（AMC23/AIME24/AIME25） | Qwen3-4B | +6.4 pp | 平均准确率由 77.2 % 提升至 83.6 %，反超 DeepSeek-R1-0120 |
| 推理期重排序 | Qwen3-4B | +9.3 pp | 无需再训练即可把平均准确率从 68.4 % 提升到 77.7 % |

---

#### 总结与反思  
* **结果总结**  
  DuPO 证明了“部分重建即可自评”的思想在翻译与数学两种截然不同的任务上均能带来稳定收益。方法省去了人工标注，突破了 RLVR 仅适用于可验证任务的限制。  
* **局限性**  
  1. 已知-未知划分目前依赖启发式规则，尚未在更开放的对话、代码生成等任务中验证；  
  2. 论文实验集中在 7B 及以下规模，超大模型上的收益与稳定性仍待评估；  
  3. 构造对偶任务与重排序会带来额外计算开销。  
* **前沿见解**  
  - 自动学习“最信息化未知组件”选择器，可让框架更通用；  
  - 与 Agent 框架结合，在多步推理、工具调用中动态生成自监督信号；  
  - 在工业场景（翻译、代码审计、数理计算）中，可作为“自检-重排序”模块直接上线，持续自优化。  

DuPO 为“自我验证大模型”提供了可行范式——只要能设计出信息可还原的对偶问题，就能让模型产生可靠的内生偏好信号，或将成为后 RLHF 时代的重要替代方案。